{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "In this notebook clustering is being applied to the correlation dataframe in order to group countries by similarity. The analysis will be performed for the different dimensions of the indicators: equality, socio-demographic and economic. In the end there is a global analysis  using all the indicators and giving a final conclusion to this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import\n",
    "\n",
    "Import all the libraries and the correlation dataframe generated in the Notebook-Golden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from numpy import sort\n",
    "from sklearn.manifold import TSNE\n",
    "import ipywidgets as widgets\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from matplotlib import gridspec\n",
    "from sklearn.cluster import OPTICS, cluster_optics_dbscan\n",
    "from sklearn.preprocessing import normalize, StandardScaler\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from ipywidgets import interact, interact_manual\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_venn import venn3\n",
    "\n",
    "output_path = os.getcwd() + '/Output/'\n",
    "col_country = 'Country'\n",
    "col_region = 'Region'\n",
    "col_year = 'Year'\n",
    "col_cluster = 'Cluster'\n",
    "col_1comp = '1st_component'\n",
    "col_2comp = '2nd_component'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOADING THE DATAFRAME\n",
    "We will use the Pearson Correlation Dataframe. We could use Spearman's by simply changing the name of the file to be read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df = pd.read_csv(output_path + 'Corr_DF_pearson.csv', index_col = col_country)\n",
    "corr_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEFINING THE GROUPS\n",
    "\n",
    "In order to carry out a more sensitive study, we will not only study the indicators as a whole, but also separate them in the following groups to study them more in-depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "econ_ind = 'Economic Indicators'\n",
    "socdem_ind = 'Social-demographic Indicators'\n",
    "eq_ind = 'Equality Indicators'\n",
    "all_ind = 'All indicators'\n",
    "\n",
    "ind_dict = {\n",
    "    econ_ind: ['CreditToAgriFishForest', 'AgriShareGDP', 'EmploymentRural', '%EmploymentAgriFishForest', 'TotalAgri', '% Soldiers', '% Healthcare Investment', '% Employment Industry', '% Education Expenditure','R&D expenditure %GDP','Researchers in R&D','Employment in agriculture','Employment in industry','Employment in services','Cost business start-up','% Education Expenditure'],\n",
    "    socdem_ind: ['Marriage Rate', 'Birth Rate', 'Death Rate', 'Homicides', 'Life Expectancy', 'Maternal Death Risk', 'Literacy Rate', 'Infant Mortality', '% Population Growth', '% Rural Population', 'Suicide Rate', 'Population'],\n",
    "    eq_ind: ['Gender Equality', 'Gender Inequality','% Men Employment', '% Women Employment', 'Women Schooling Years', 'Men Schooling Years', 'Freedom of Expression', '% Undernourishment', 'Civil Liberties', 'Gini','Tertiary School Gender Parity','% Female Employment','% Male Employment','% Vulnerable male employment','% Vulnerable female employment'],\n",
    "    all_ind: corr_df.columns.tolist()\n",
    "}\n",
    "\n",
    "# Keep only the ones that have been carried on to this point.\n",
    "\n",
    "indicators = set(corr_df.columns)\n",
    "for ind in ind_dict:\n",
    "    ind_dict[ind] = list(set(ind_dict[ind]) & indicators)\n",
    "\n",
    "# Divide the corr_df into slices for each group of indicators, normalize them and store them into a dict.\n",
    "df_dict = {}\n",
    "for ind in ind_dict:\n",
    "    df_norm = corr_df.copy()\n",
    "    df_norm.drop(df_norm.columns.difference(ind_dict[ind]), axis = 'columns', inplace=True)\n",
    "    df_norm = df_norm.dropna(how = 'all').fillna(value = 0)\n",
    "    df_norm.name = ind\n",
    "    df_dict[ind] = df_norm\n",
    "\n",
    "for ind in ind_dict:\n",
    "    print(df_dict[ind].name)\n",
    "    display(df_dict[ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-SNE\n",
    "\n",
    "The algorithm of t-SNE (t-Distributed Stochastic Neighbor Embedding) is used to reduce the dimensionality of all the indicators to only 2 components. It admits a number of parameters to tune the procedure and obtain an optimal output: \n",
    "- N-components: the dimensions to analyze.\n",
    "- Perplexity: measurement of how well a probability distribution or probability model predicts a sample.\n",
    "- N-iter: iterations for the optimization\n",
    "- Learning rate: how fast the algorithm learns. A high value may cause a ball form of the data.\n",
    "- Init: Initialization of embedding. 'pca' is the new default for newer versions.\n",
    "\n",
    "In this cell, we define the different values that will be used for each group of indicators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The TSNE algorithm admits a number of parameters\n",
    "tsne_dict = {\n",
    "    econ_ind: TSNE(n_components = 2, perplexity = 5, n_iter = 20000, learning_rate = 100.0, init = 'pca'),\n",
    "    socdem_ind: TSNE(n_components = 2, perplexity = 5, n_iter = 20000, learning_rate = 100.0, init = 'pca'),\n",
    "    eq_ind: TSNE(n_components = 2, perplexity = 5, n_iter = 20000, learning_rate = 100.0, init = 'pca'),\n",
    "    all_ind: TSNE(n_components = 2, perplexity = 5, n_iter = 20000, learning_rate = 100.0, init = 'pca')\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLUSTERING\n",
    "\n",
    "### Affinity Propagation\n",
    "\n",
    "The clustering method used in the notebook is Affinity Propagation. The reason behind choosing this one is because it is suited for our data: many clusters with uneven cluster size; also, it can simply be optmized changing 1 parameter, \"damping\". \n",
    "- Damping: is the extent to which the current value is maintained relative to incoming values (weighted 1 - damping).\n",
    "\n",
    "Again, in this cell we will calibrate the clustering algorithm for each indicator group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "afprop_dict = {\n",
    "    econ_ind: AffinityPropagation(damping=0.5),\n",
    "    socdem_ind: AffinityPropagation(damping=0.5),\n",
    "    eq_ind: AffinityPropagation(damping=0.5),\n",
    "    all_ind: AffinityPropagation(damping=0.5)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for ind in ind_dict:\n",
    "    df_norm = df_dict[ind]\n",
    "\n",
    "    # Apply the TSNE chosen for that indicator\n",
    "    df_tsne = pd.DataFrame(tsne_dict[ind].fit_transform(df_norm))\n",
    "\n",
    "    # Scale the result\n",
    "    scaled_df = pd.DataFrame(StandardScaler().fit_transform(df_tsne), index = df_norm.index, columns = [col_1comp, col_2comp])\n",
    "    \n",
    "    # Apply Affinity Propagation\n",
    "    affinity = afprop_dict[ind].fit(scaled_df)\n",
    "\n",
    "    # Update the DataFrame with the resulting Cluster labels\n",
    "    labels_affinity = affinity.labels_\n",
    "    df_dict[ind].loc[:, col_cluster] = labels_affinity\n",
    "    scaled_df[col_cluster] = labels_affinity\n",
    "    scaled_df[col_cluster] = scaled_df[col_cluster].astype(str)\n",
    "\n",
    "    # Show the resulting chart\n",
    "    fig = px.scatter(scaled_df, x = col_1comp, y = col_2comp, text = scaled_df.index, size_max=100, color=col_cluster, category_orders={col_cluster: sort(list(set(scaled_df.loc[:, col_cluster])))})\n",
    "    fig.update_layout(title_text=ind, title_x=0.5)\n",
    "    fig.update_traces(textposition='top center')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Widget Clustering\n",
    "\n",
    "In order to explore the clustering results given a country, we can choose that country and the group of indicators in the table below, to see what other countries are in the same cluster for the selected group of indicators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tableCountry(Ind, Country):\n",
    "    try:\n",
    "        # Find the cluster Country belongs to.\n",
    "        cluster_number = df_dict[Ind].loc[df_dict[Ind].index == Country, col_cluster].item()\n",
    "\n",
    "        # Retrieve the Dataframe with the selected indicators, and filter to only show the rows (countries) belonging to the cluster. Drop the cluster number column as it is redundant.\n",
    "        df_ind = df_dict[Ind]\n",
    "        df = df_ind.loc[df_ind[col_cluster] == cluster_number].drop(col_cluster, axis = 'columns')\n",
    "\n",
    "        # Format the Dataframe representation.\n",
    "        df_s = df.style\n",
    "        df_s.apply_index(lambda i: ['background-color: #aadfff; font-weight: 500' if c == Country else '' for c in i], axis = 0)\n",
    "        df_s.apply(lambda row: ['background-color: #ccebff;' if row.name == Country else '' for cell in row], axis = 1)\n",
    "        df_s.set_table_styles([{'selector': 'td:hover', 'props': [('background-color', '#ddfdff')]}])\n",
    "        tt = {}\n",
    "        for col in df.columns:\n",
    "            tt[col] = 'Column median: ' + str(df.loc[:, col].median())\n",
    "        df_s.set_tooltips(pd.DataFrame(tt, index = df.index))\n",
    "\n",
    "        # Display a short descriptive title and the Dataframe.\n",
    "        display(Country + ' belongs to Cluster ' + str(cluster_number) + '. This Cluster contains a total of ' + str(df.shape[0]) + ' countries.')\n",
    "        display(df_s)\n",
    "    except Exception:\n",
    "        return print('No indicators available for this country.')\n",
    "\n",
    "@interact(\n",
    "    Indicators = df_dict.keys(),\n",
    "    Country = sort(corr_df.index.tolist()))\n",
    "\n",
    "def g(Indicators = 'Equality indicators', Country = 'Afghanistan'):\n",
    "    return tableCountry(Indicators, Country)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAVING CLUSTERING RESULTS\n",
    "\n",
    "We now write a .csv file for each group of indicators and the clusters each countries belongs to. By default, we sort the rows by cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_folder = os.getcwd() + '/Output/Cluster/'\n",
    "\n",
    "if not os.path.exists(cluster_folder):\n",
    "            os.makedirs(cluster_folder)\n",
    "\n",
    "for ind in ind_dict:\n",
    "    print(ind)\n",
    "    df = df_dict[ind]\n",
    "    df = df.set_index(['Cluster', df.index]).sort_index()\n",
    "    df.to_csv(cluster_folder + ind + '.csv')\n",
    "    display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VENN DIAGRAM\n",
    "\n",
    "Finally as a more visual way to represent the clustering, we show a Venn diagram for any given country so we can observe all the countries that are related to it based on the different indicators group.\n",
    "\n",
    "NOTE: the central intersection does not correspond to the all indicators clustering, as they have obtained by different methods. The intersection is more restrictive and contains fewer countries than the all indicators group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VennOut(Country):\n",
    "\n",
    "    set_econ = set(df_dict[econ_ind].loc[lambda df: df[col_cluster] == df.loc[Country, col_cluster]].index.to_list())\n",
    "    set_socdem = set(df_dict[socdem_ind].loc[lambda df: df[col_cluster] == df.loc[Country, col_cluster]].index.to_list())\n",
    "    set_eq = set(df_dict[eq_ind].loc[lambda df: df[col_cluster] == df.loc[Country, col_cluster]].index.to_list())\n",
    "\n",
    "    venn = venn3([set_econ, set_socdem, set_eq], (econ_ind, socdem_ind, eq_ind))\n",
    "\n",
    "    venn.get_label_by_id('100').set_text('\\n'.join(set_econ - set_socdem - set_eq)) # Only econ\n",
    "    venn.get_label_by_id('010').set_text('\\n'.join(set_socdem - set_econ - set_eq)) # Only socdem\n",
    "    venn.get_label_by_id('001').set_text('\\n'.join(set_eq - set_econ - set_socdem)) # Only eq\n",
    "\n",
    "    # The three pair-intersections is guaranteed only if there is an intersection of the three groups.\n",
    "    if len(set_econ & set_socdem & set_eq):\n",
    "        venn.get_label_by_id('111').set_text('\\n'.join(set_econ & set_socdem & set_eq))\n",
    "        venn.get_label_by_id('110').set_text('\\n'.join(set_econ & set_socdem - set_eq))\n",
    "        venn.get_label_by_id('101').set_text('\\n'.join(set_econ & set_eq - set_socdem))\n",
    "        venn.get_label_by_id('011').set_text('\\n'.join(set_socdem & set_eq - set_econ))\n",
    "    else:\n",
    "        # If no center, check the intersections that do exist.\n",
    "        if len(set_econ & set_socdem - set_eq):\n",
    "            venn.get_label_by_id('110').set_text('\\n'.join(set_econ & set_socdem - set_eq))\n",
    "        if len(set_econ & set_eq - set_socdem):\n",
    "            venn.get_label_by_id('101').set_text('\\n'.join(set_econ & set_eq - set_socdem))\n",
    "        if len(set_socdem & set_eq - set_econ):\n",
    "            venn.get_label_by_id('011').set_text('\\n'.join(set_socdem & set_eq - set_econ))\n",
    "\n",
    "    plt.rcParams[\"figure.figsize\"] = (12, 12)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "@interact(\n",
    "    Country = sort(corr_df.index.tolist()))\n",
    "\n",
    "def g(Country = 'Spain'):\n",
    "    return VennOut(Country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_optics = pd.DataFrame(StandardScaler().fit_transform(new_df_tsne))\n",
    "\n",
    "'''Apply OPTICS'''\n",
    "optics = OPTICS(xi=.35, min_cluster_size=3, min_samples=5).fit_predict(X_optics)\n",
    "#labels_optics = optics.labels_\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.title('OPTICS',fontsize= 20)\n",
    "plt.xlabel('Feature 1',fontsize= 18)\n",
    "plt.ylabel('Feature 2',fontsize= 18)\n",
    "fig = plt.scatter(X_optics[0], X_optics[1], c= optics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_kmeans = pd.DataFrame(StandardScaler().fit_transform(new_df_tsne))\n",
    "X_kmeans.index = new_df_tsne.index\n",
    "'''Apply K-Means'''\n",
    "from sklearn.cluster import *\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "\n",
    "kmean_clusters =   MiniBatchKMeans(n_clusters=6).fit_predict(X_kmeans)\n",
    "\n",
    "plt.figure(figsize = (8,8))\n",
    "plt.title('K-Means Clustering',fontsize= 20)\n",
    "plt.xlabel('Feature 1', fontsize=18)\n",
    "plt.ylabel('Feature 2', fontsize=18)\n",
    "f = plt.scatter(X_kmeans[0],X_kmeans[1],c=kmean_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(20, 20)})\n",
    "\n",
    "\n",
    "z = X_kmeans[0]\n",
    "y = X_kmeans[1]\n",
    "n = new_df_tsne.index.get_level_values(0)\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(z, y, c=kmean_clusters)\n",
    "\n",
    "\n",
    "\n",
    "for i, txt in enumerate(n):\n",
    "    ax.annotate(txt, (z[i], y[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" new_df_tsne['Cluster'] = kmean_clusters.tolist()\n",
    "new_df_tsne \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "new_df_tsne.to_csv(write_path + '/Cluster.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "6044c39f3fa8d69f78786198aef61ed0dc3fdd7ddd5a88c111ee27e3b3325eb4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
