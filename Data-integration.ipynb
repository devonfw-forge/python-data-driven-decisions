{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries and functions.\n",
    "\n",
    "</p>Search for archives, and create the 1st data frame, as well as an empty list for the headers (h) and the list for comun columns (CC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "from pyspark.sql.functions import concat, col, lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\amarchve\\\\Documents\\\\GitHub\\\\python-data-driven-decisions\\\\.venv\\\\Scripts;C:\\\\Users\\\\amarchve\\\\Documents\\\\GitHub\\\\python-data-driven-decisions\\\\.venv\\\\Scripts;C:\\\\Program Files\\\\Eclipse Foundation\\\\jdk-11.0.12.7-hotspot\\\\bin;C:\\\\WINDOWS\\\\system32;C:\\\\WINDOWS;C:\\\\WINDOWS\\\\System32\\\\Wbem;C:\\\\WINDOWS\\\\System32\\\\WindowsPowerShell\\\\v1.0\\\\;C:\\\\WINDOWS\\\\System32\\\\OpenSSH\\\\;C:\\\\WINDOWS\\\\system32\\\\config\\\\systemprofile\\\\.poetry\\\\bin;C:\\\\Program Files\\\\hadoop-3.2.0\\\\bin;C:\\\\Users\\\\amarchve\\\\.poetry\\\\bin;C:\\\\Users\\\\amarchve\\\\AppData\\\\Local\\\\Microsoft\\\\WindowsApps;C:\\\\Users\\\\amarchve\\\\AppData\\\\Local\\\\Programs\\\\Microsoft VS Code\\\\bin;C:\\\\Users\\\\amarchve\\\\AppData\\\\Local\\\\GitHubDesktop\\\\bin;C:\\\\Users\\\\amarchve\\\\AppData\\\\Local\\\\Programs\\\\Git\\\\cmd'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYSPARK_DRIVER_PYTHON=jupyter\n",
      "env: PYSPARK_PYTHON=python\n"
     ]
    }
   ],
   "source": [
    "%env PYSPARK_DRIVER_PYTHON jupyter\n",
    "%env PYSPARK_PYTHON python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PATH'] = os.environ['PATH']+ \";C:\\\\Program Files\\\\hadoop-3.2.0\\\\bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"JAVA_HOME\"] = \"C:\\\\Program Files\\\\Eclipse Foundation\\\\jdk-11.0.12.7-hotspot\"\n",
    "os.environ[\"HADOOP_HOME\"] = \"C:\\\\Program Files\\\\hadoop-3.2.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Using the <code>glob</code> library allows us to create a list with only the csv that are ended in <em>(Normalized).csv</em> , which will be the most useful for a statistical analysis. <br>\n",
    "Moreover, for a future simplification, we also create a list with the relevant information of each of the csv <em>('Area','Year','Element','Unit','Value')</em> </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = glob.glob(os.getcwd()+ \"/Data/\"+\"/**(Normalized).csv\")\n",
    "CC=['Area','Year','Element' or 'Item','Unit','Value']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intitiate the spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "  \n",
    "ss = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Our First Spark example\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will allow to read  the csv files from the main directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+--------------------------------------------------------------------------------------------+---------+\n",
      "|Area               |Year|Unique                                                                                      |Value    |\n",
      "+-------------------+----+--------------------------------------------------------------------------------------------+---------+\n",
      "|Algeria            |2009|Agriculture research spending - Share of Value Added (Agriculture, Forestry and Fishing) - %|0.180000 |\n",
      "|Algeria            |2010|Agriculture research spending - Share of Value Added (Agriculture, Forestry and Fishing) - %|0.180000 |\n",
      "|Algeria            |2011|Agriculture research spending - Share of Value Added (Agriculture, Forestry and Fishing) - %|0.210000 |\n",
      "|Algeria            |2012|Agriculture research spending - Share of Value Added (Agriculture, Forestry and Fishing) - %|0.210000 |\n",
      "|Algeria            |2009|Agriculture research spending - Spending, total - million PPP (constant 2011 prices)        |76.900000|\n",
      "|Algeria            |2010|Agriculture research spending - Spending, total - million PPP (constant 2011 prices)        |71.000000|\n",
      "|Algeria            |2011|Agriculture research spending - Spending, total - million PPP (constant 2011 prices)        |82.900000|\n",
      "|Algeria            |2012|Agriculture research spending - Spending, total - million PPP (constant 2011 prices)        |91.600000|\n",
      "|Antigua and Barbuda|2007|Agriculture research spending - Share of Value Added (Agriculture, Forestry and Fishing) - %|4.200000 |\n",
      "|Antigua and Barbuda|2008|Agriculture research spending - Share of Value Added (Agriculture, Forestry and Fishing) - %|3.810000 |\n",
      "|Antigua and Barbuda|2009|Agriculture research spending - Share of Value Added (Agriculture, Forestry and Fishing) - %|4.020000 |\n",
      "|Antigua and Barbuda|2010|Agriculture research spending - Share of Value Added (Agriculture, Forestry and Fishing) - %|3.680000 |\n",
      "|Antigua and Barbuda|2011|Agriculture research spending - Share of Value Added (Agriculture, Forestry and Fishing) - %|3.650000 |\n",
      "|Antigua and Barbuda|2012|Agriculture research spending - Share of Value Added (Agriculture, Forestry and Fishing) - %|2.980000 |\n",
      "|Antigua and Barbuda|2007|Agriculture research spending - Spending, total - million PPP (constant 2011 prices)        |1.500000 |\n",
      "|Antigua and Barbuda|2008|Agriculture research spending - Spending, total - million PPP (constant 2011 prices)        |1.300000 |\n",
      "|Antigua and Barbuda|2009|Agriculture research spending - Spending, total - million PPP (constant 2011 prices)        |1.200000 |\n",
      "|Antigua and Barbuda|2010|Agriculture research spending - Spending, total - million PPP (constant 2011 prices)        |1.100000 |\n",
      "|Antigua and Barbuda|2011|Agriculture research spending - Spending, total - million PPP (constant 2011 prices)        |1.300000 |\n",
      "|Antigua and Barbuda|2012|Agriculture research spending - Spending, total - million PPP (constant 2011 prices)        |1.000000 |\n",
      "+-------------------+----+--------------------------------------------------------------------------------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file0 = ss.read.csv(file_list[0], sep=',',header= True)\n",
    "file0=file0.select('Area','Year', (concat(col('Item'),lit(' - '), col('Element'),lit(' - '),col('Unit'))).alias('Unique'),'Value')\n",
    "file0.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop attempt; not everyone has element or item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (1,len(file_list)):\n",
    "    file1=ss.read.csv(file_list[i], header=True)\n",
    "    file1=file1.select('Area','Year', (concat(col('Item'),lit(' - '), col('Element'),lit(' - '),col('Unit'))).alias('Unique'),'Value')\n",
    "    file0=file0.union(file1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Loop attempt2; looks promising, not finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1332274217.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [12]\u001b[1;36m\u001b[0m\n\u001b[1;33m    if i == #put number where the damm archive is:\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "for i in range (1,len(file_list)):\n",
    "    if i == #put number where the damm archive is:\n",
    "        continue\n",
    "    else:\n",
    "        file1=ss.read.csv(file_list[i], header=True)\n",
    "        if 'Element' in file1.schema.fieldNames():\n",
    "            if 'Item' in file1.schema.fieldNames():\n",
    "                if 'Area' in file1.schema.fieldNames():\n",
    "                    pass \n",
    "                else:\n",
    "                    file1=file1.withColumn('Area',col('Recipient Country'))\n",
    "            else:\n",
    "                file1=file1.withColumn('Item', lit(0))\n",
    "        else:\n",
    "            file1=file1.withColumn('Element', lit(0))\n",
    "            if 'Item' in file1.schema.fieldNames():\n",
    "                pass\n",
    "            else:\n",
    "                file1=file1.withColumn('Item', lit(0))\n",
    "        file1=file1.select('Area' or 'Country','Year', (concat(col('Item'),lit(' - '), col('Element'),lit(' - '),col('Unit'))).alias('Unique'),'Value')\n",
    "        file0=file0.union(file1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Area Code', 'Item Code', 'Element Code', 'Year Code', 'Source Code']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file1=ss.read.csv(file_list[18], header=True)\n",
    "[element for element in file1.columns if 'Cod' in element]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data integration through pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As at the beginning we do not have enough computer power to process the whole database, we are going to develop a test run to be sure that the ideas are escalable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first test run for the loop, we are going to load and process the data into concated data frames, where later on, there is an application of the <code>pivot_table</code> function which adjusts all the variables, previouly called 'Elements' & 'Units' into the headers of the columns, and the values the values of the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dataframe = pd.DataFrame(pd.read_csv(file_list[0], sep=',', encoding='latin-1'),columns=CC)\n",
    "for i in range(1,5):\n",
    "    df = pd.DataFrame(pd.read_csv(file_list[i],sep=',' , encoding='latin-1',low_memory=False), columns=CC)\n",
    "    main_dataframe = pd.concat([main_dataframe, df])\n",
    "\n",
    "main_dataframeC=main_dataframe.pivot_table(index=['Area','Year'], columns= ['Element' or 'Item','Unit'], values='Value')\n",
    "main_dataframeC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>In the following cell, we are concatanating all the files from the <code>file_list</code>, which will have the same shape thanks to creation of the dataframes with the restriction of the columns. <br>\n",
    "Moreover, this concat function will allow for a single data frame which has all the files one on top of another. Therefore the final result form this loop will be <code>main_dataframe</code> which will be our Normalized Source Data Model.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dataframe = pd.DataFrame(pd.read_csv(file_list[0], sep=',', encoding='latin-1'),columns=CC)\n",
    "for i in range(1,len(file_list)):\n",
    "    df = pd.DataFrame(pd.read_csv(file_list[i],sep=',' , encoding='latin-1',low_memory=False), columns=CC)\n",
    "    main_dataframe = pd.concat([main_dataframe, df])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Lastly, to convert the Normalized Source Data Model into the Normalized Integrated Data Model, we are going to use the <code>pivot_table</code> function which allows to <br>\n",
    "adjusts all the variables, previouly called <em>'Elements' & 'Units'</em> into the headers of the columns, and the <em>'Value'</em> column will be the values of the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dataframeC=main_dataframe.pivot_table(index=['Area','Year'], columns= ['Element' or 'Item' ,'Unit'], values='Value')\n",
    "main_dataframeC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quality assurance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we are going to make sure that none of our interesting variables from <em>'Elements'</em> have been left out, thus checking if the extraction & integtration has been completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data extraction & integration is COMPLETED and CORRECT\n"
     ]
    }
   ],
   "source": [
    "if len(main_dataframe[\"Element\" or 'Item'].value_counts())==main_dataframeC.shape[1]:\n",
    "    print('Data extraction & integration is COMPLETED and CORRECT')\n",
    "else:\n",
    "    print('Data extraction & integration is UNCOMPLETED')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad72b07258a3fa24452fee21e868a537ead700a3a6ac2a1adaf5006160c747dc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
